# HRM Models - Distributed Training Guide

Este documento explica c√≥mo usar el sistema de entrenamiento distribuido para los modelos HRM 50M y 100M en m√∫ltiples GPUs.

## üöÄ Caracter√≠sticas Principales

- **Entrenamiento Distribuido**: Usa `torch.distributed` con DistributedDataParallel (DDP)
- **Auto-configuraci√≥n**: Detecta autom√°ticamente GPUs y configura batch sizes √≥ptimos
- **Balanceado de Carga**: Distribuye datos y c√≥mputo equitativamente entre GPUs
- **Sincronizaci√≥n de Gradientes**: Optimizada para modelos HRM con adaptive computation
- **Memoria Eficiente**: Gradient checkpointing para modelo 100M
- **Monitoreo Avanzado**: Progress bars, m√©tricas HRM, y uso de memoria GPU

## üìã Requisitos del Sistema

### Hardware M√≠nimo
- **Para HRM 50M**: 2+ GPUs con 8GB+ VRAM cada una
- **Para HRM 100M**: 2+ GPUs con 12GB+ VRAM cada una

### Hardware Recomendado
- **Para HRM 50M**: 4-8 GPUs con 12GB+ VRAM cada una
- **Para HRM 100M**: 4-8 GPUs con 16GB+ VRAM cada una

### Software
```bash
# Dependencias principales
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers datasets tokenizers tqdm

# Dependencias opcionales para mejor rendimiento
pip install flash-attn  # Para Flash Attention (opcional)
```

## üèóÔ∏è Estructura de Archivos

```
HRM_Models/
‚îú‚îÄ‚îÄ hrm_training_small_50m_distributed.py      # Script para modelo 50M
‚îú‚îÄ‚îÄ hrm_training_medium_100m_distributed.py    # Script para modelo 100M  
‚îú‚îÄ‚îÄ launch_distributed_training.py             # Launcher autom√°tico
‚îú‚îÄ‚îÄ hf_tokenizer_wrapper_simple.py            # Wrapper tokenizador
‚îî‚îÄ‚îÄ README_DISTRIBUTED_TRAINING.md            # Esta gu√≠a
```

## üöÄ Uso R√°pido

### M√©todo 1: Launcher Autom√°tico (Recomendado)

El launcher detecta autom√°ticamente tu configuraci√≥n de GPUs y optimiza los par√°metros:

```bash
# Entrenamiento 50M con auto-configuraci√≥n
python launch_distributed_training.py --model 50m

# Entrenamiento 100M con 4 GPUs espec√≠ficas
python launch_distributed_training.py --model 100m --gpus 4

# Con configuraci√≥n personalizada
python launch_distributed_training.py --model 50m --gpus 2 --train_samples 200000

# Dry run para ver la configuraci√≥n sin entrenar
python launch_distributed_training.py --model 100m --dry_run
```

### M√©todo 2: Torchrun Manual

Para control completo, puedes usar torchrun directamente:

```bash
# HRM 50M con 2 GPUs
torchrun --nproc_per_node=2 hrm_training_small_50m_distributed.py \
    --batch_size 6 --train_samples 100000

# HRM 100M con 4 GPUs
torchrun --nproc_per_node=4 hrm_training_medium_100m_distributed.py \
    --batch_size 3 --train_samples 200000 --learning_rate 1e-5
```

## ‚öôÔ∏è Configuraciones Recomendadas

### HRM Small 50M (~50M par√°metros)

| GPUs | Batch Size/GPU | Batch Total | VRAM/GPU | Throughput |
|------|----------------|-------------|----------|------------|
| 2    | 6-8            | 12-16       | 8-12GB   | ~2000 tok/s |
| 4    | 4-6            | 16-24       | 8-10GB   | ~4000 tok/s |
| 8    | 2-4            | 16-32       | 6-8GB    | ~6000 tok/s |

```bash
# Configuraci√≥n √≥ptima para diferentes GPU counts
# 2 GPUs
torchrun --nproc_per_node=2 hrm_training_small_50m_distributed.py --batch_size 6

# 4 GPUs  
torchrun --nproc_per_node=4 hrm_training_small_50m_distributed.py --batch_size 4

# 8 GPUs
torchrun --nproc_per_node=8 hrm_training_small_50m_distributed.py --batch_size 3
```

### HRM Medium 100M (~100M par√°metros)

| GPUs | Batch Size/GPU | Batch Total | VRAM/GPU | Throughput |
|------|----------------|-------------|----------|------------|
| 2    | 3-4            | 6-8         | 12-16GB  | ~1000 tok/s |
| 4    | 2-3            | 8-12        | 10-12GB  | ~2000 tok/s |
| 8    | 1-2            | 8-16        | 8-10GB   | ~3000 tok/s |

```bash
# Configuraci√≥n √≥ptima para diferentes GPU counts
# 2 GPUs (recomendado 16GB+ VRAM)
torchrun --nproc_per_node=2 hrm_training_medium_100m_distributed.py --batch_size 3

# 4 GPUs (recomendado 12GB+ VRAM)
torchrun --nproc_per_node=4 hrm_training_medium_100m_distributed.py --batch_size 2

# 8 GPUs (m√≠nimo 8GB VRAM)
torchrun --nproc_per_node=8 hrm_training_medium_100m_distributed.py --batch_size 1
```

## üìä Par√°metros de Entrenamiento

### Par√°metros Comunes

```bash
# Dataset
--dataset_name allenai/c4          # Dataset base
--dataset_config en               # Idioma
--train_samples 100000            # Samples entrenamiento
--val_samples 10000               # Samples validaci√≥n

# Tokenizador
--tokenizer openai-community/gpt2 # Tokenizador base
--max_text_length 2000            # Longitud m√°xima texto
--min_text_length 50              # Longitud m√≠nima texto

# Entrenamiento
--epochs 3                        # N√∫mero de √©pocas
--learning_rate 3e-5              # Learning rate
--batch_size 4                    # Batch size por GPU
--save_steps 500                  # Frecuencia de guardado
--eval_steps 100                  # Frecuencia de evaluaci√≥n

# Optimizaci√≥n
--fast_mode                       # Descarga completa dataset
--no_streaming                    # No usar streaming
```

### Par√°metros Espec√≠ficos HRM 50M

```bash
# Configuraci√≥n optimizada para 50M
--learning_rate 3e-5
--train_samples 100000
--val_samples 10000
--max_text_length 2000
--save_steps 500
--eval_steps 100
```

### Par√°metros Espec√≠ficos HRM 100M

```bash
# Configuraci√≥n optimizada para 100M
--learning_rate 1e-5              # LR m√°s conservador
--train_samples 200000            # M√°s datos
--val_samples 20000
--max_text_length 2048            # Context m√°s largo
--min_text_length 100             # Textos m√≠nimos m√°s largos
--save_steps 1000                 # Checkpoints menos frecuentes
--eval_steps 200
```

## üîß Arquitectura y Optimizaciones

### Distributed Data Parallel (DDP)
- Usa `torch.nn.parallel.DistributedDataParallel` en lugar de `DataParallel`
- Sincronizaci√≥n de gradientes optimizada con `find_unused_parameters=True` para HRM
- Comunicaci√≥n NCCL para GPUs, Gloo para CPU

### Divisi√≥n de Datos
- Cada GPU procesa un subset diferente del dataset
- `DistributedSampler` maneja la distribuci√≥n autom√°ticamente
- Sincronizaci√≥n entre √©pocas con `sampler.set_epoch(epoch)`

### Memoria y Rendimiento
- **Gradient Checkpointing**: Activado para modelo 100M
- **Mixed Precision**: AMP con `torch.cuda.amp` para mejor rendimiento
- **Pin Memory**: Para transferencias CPU‚ÜíGPU m√°s r√°pidas
- **Gradient Clipping**: M√°s agresivo para modelos grandes

### Caracter√≠sticas HRM Espec√≠ficas
- **Adaptive Computation**: Q-learning para halting autom√°tico
- **Deep Supervision**: P√©rdidas intermedias de capas HRM
- **Ponder Loss**: Regularizaci√≥n computacional
- **Hierarchical Cycles**: H_cycles y L_cycles configurables

## üìà Monitoreo y M√©tricas

### M√©tricas de Entrenamiento
- **Loss Components**: Main loss, Deep supervision, Ponder loss, Q-learning loss
- **HRM Metrics**: H-updates, L-steps promedio, convergencia rate
- **GPU Metrics**: Memoria usada/reservada por GPU
- **Training Speed**: Samples/segundo total (suma de todas las GPUs)

### Progress Tracking
- Barra de progreso con `tqdm` (solo proceso principal)
- Logging detallado cada 50 steps
- Evaluaci√≥n peri√≥dica con early stopping
- Checkpoints autom√°ticos y best model saving

## üêõ Troubleshooting

### Problemas Comunes

#### 1. Out of Memory (OOM)
```bash
# Soluci√≥n: Reducir batch size
--batch_size 2  # En lugar de 4

# O usar gradient checkpointing (100M tiene por defecto)
# 50M: Editar config.gradient_checkpointing=True
```

#### 2. NCCL Timeout/Hanging
```bash
# Soluci√≥n: Variables de entorno
export NCCL_DEBUG=INFO
export NCCL_TIMEOUT=1800
export NCCL_IB_DISABLE=1  # Si hay problemas con InfiniBand
```

#### 3. Port Already in Use
```bash
# Soluci√≥n: Cambiar puerto master
torchrun --nproc_per_node=4 --master_port=29501 script.py
```

#### 4. NaN/Inf en Loss
- Los scripts tienen protecci√≥n contra NaN incorporada
- Se salta optimizaci√≥n si se detectan NaN
- Loss components se estabilizan autom√°ticamente

### Debugging

```bash
# Modo verbose para m√°s informaci√≥n
python launch_distributed_training.py --model 50m --verbose

# Ver configuraci√≥n sin entrenar
python launch_distributed_training.py --model 50m --dry_run

# Variables de entorno √∫tiles
export CUDA_LAUNCH_BLOCKING=1  # Para debugging
export NCCL_DEBUG=INFO         # Para debugging NCCL
export TOKENIZERS_PARALLELISM=false  # Evitar warnings
```

## üìä Benchmarks de Rendimiento

### HRM 50M (4x RTX 4090)
- **Throughput**: ~4,000 tokens/segundo
- **Memory/GPU**: ~8GB VRAM
- **Batch Size Total**: 16 (4 per GPU)
- **Training Time**: ~6 horas para 100K samples

### HRM 100M (4x A100 40GB)
- **Throughput**: ~2,000 tokens/segundo  
- **Memory/GPU**: ~12GB VRAM
- **Batch Size Total**: 8 (2 per GPU)
- **Training Time**: ~12 horas para 200K samples

## üéØ Mejores Pr√°cticas

### 1. Configuraci√≥n de Hardware
- Usar GPUs del mismo tipo para mejor balancing
- Asegurar conectividad PCIe/NVLink para comunicaci√≥n r√°pida
- Monitorear temperaturas durante entrenamientos largos

### 2. Configuraci√≥n de Datos
- Usar `--no_streaming` para datasets grandes (mejor rendimiento)
- Ajustar `--train_samples` basado en memoria disponible
- Usar textos m√°s largos para modelo 100M (`--max_text_length 2048`)

### 3. Optimizaci√≥n de Entrenamiento
- Empezar con configuraciones recomendadas y ajustar gradualmente
- Usar launcher autom√°tico para configuraci√≥n inicial √≥ptima
- Monitorear m√©tricas HRM (H-updates, L-steps) para validar funcionamiento

### 4. Checkpoints y Resuming
- Los checkpoints se guardan solo en el proceso principal (rank 0)
- Para resumir entrenamiento, cargar modelo desde checkpoint guardado
- Best model se guarda autom√°ticamente basado en validation loss

## üìû Soporte y Contribuciones

Para reportar problemas o sugerir mejoras:
1. Verificar que el problema no est√© en la secci√≥n de Troubleshooting
2. Incluir informaci√≥n de sistema: GPU models, VRAM, driver versions
3. Incluir logs completos y configuraci√≥n usada
4. Mencionar si el problema es espec√≠fico a distributed training

---

üöÄ **¬°Happy Training!** üöÄ