# -*- coding: utf-8 -*-
"""
HRM-Models Distributed Training Script - MODELO SMALL ~50M PAR√ÅMETROS
VERSI√ìN MULTI-GPU CORREGIDA: Distribuci√≥n de datos CORRECTA entre GPUs

üñ•Ô∏è  CARACTER√çSTICAS:
- Entrenamiento distribuido con torch.distributed
- DistributedDataParallel (DDP) para mejor escalabilidad
- Divisi√≥n CORRECTA de datos entre GPUs - cada GPU procesa datos √∫nicos
- Sincronizaci√≥n de gradientes optimizada
- Balanceado de carga inteligente
"""

import os, multiprocessing as mp, math, time
from typing import List, Dict, Optional, Tuple
import argparse

# Progress bar
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("‚ö†Ô∏è tqdm no disponible, usando progreso b√°sico")

# Configurar m√©todo de multiprocessing antes de cualquier uso
if __name__ == '__main__':
    # Usar spawn en lugar de fork para evitar pickle issues
    mp.set_start_method('spawn', force=True)
    # Marcar PID principal para evitar spam en multiprocessing
    import os
    os._main_pid = os.getpid()

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, IterableDataset
from torch.optim import AdamW

# Importar wrapper de tokenizador HF simplificado
try:
    from hf_tokenizer_wrapper_simple import HuggingFaceTokenizerWrapper, create_tokenizer
    HF_TOKENIZER_AVAILABLE = True
    print("‚úÖ HuggingFace tokenizer wrapper simple disponible")
except ImportError:
    HF_TOKENIZER_AVAILABLE = False
    print("‚ùå HuggingFace tokenizer wrapper NO disponible")
    print("üí° Ejecute: pip install transformers tokenizers")
    exit(1)

# [TODAS LAS CLASES HRM PERMANECEN IGUALES - solo copio las partes relevantes aqu√≠]
# Para ahorrar espacio, asumo que las clases HRM est√°n disponibles

def setup_distributed():
    """Initialize distributed training"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        dist.init_process_group(
            backend='nccl' if torch.cuda.is_available() else 'gloo',
            init_method='env://',
            rank=rank,
            world_size=world_size
        )
        
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)
            device = torch.device(f'cuda:{local_rank}')
        else:
            device = torch.device('cpu')
            
        return rank, world_size, local_rank, device
    else:
        return 0, 1, 0, torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def cleanup_distributed():
    """Clean up distributed training"""
    if dist.is_initialized():
        dist.destroy_process_group()

# ======= FUNCI√ìN CORREGIDA PARA DISTRIBUCI√ìN DE DATOS =======
def load_dataset_hf_fixed(tokenizer, split: str = "train", num_samples: int = 1000,
                   dataset_name: str = "allenai/c4", dataset_config: str = "en",
                   text_column: str = "text", min_text_length: int = 50,
                   max_text_length: int = 2000, use_streaming: bool = True, 
                   fast_mode: bool = False, rank: int = 0, world_size: int = 1):
    """üîß VERSI√ìN CORREGIDA: Cada rank carga solo SU porci√≥n √∫nica del dataset"""
    try:
        from datasets import load_dataset

        # ‚úÖ CORRECCI√ìN: Calcular samples por rank ANTES de cargar
        samples_per_rank = num_samples // world_size
        start_sample = rank * samples_per_rank
        end_sample = start_sample + samples_per_rank
        
        # El √∫ltimo rank toma samples restantes
        if rank == world_size - 1:
            end_sample = num_samples

        if rank == 0:
            print(f"üì• üîß DISTRIBUCI√ìN CORREGIDA - Dataset '{dataset_name}':")
            print(f"   üìä Total samples: {num_samples}")
            print(f"   üßÆ Samples por rank: {samples_per_rank}")
            print(f"   üåê World size: {world_size}")
        
        print(f"   üéØ Rank {rank}: samples {start_sample}-{end_sample} ({end_sample-start_sample} samples √öNICOS)")

        if fast_mode and num_samples > 50000:
            if rank == 0:
                print("‚ö° Fast mode: Usando dataset no-streaming")
            use_streaming = False

        # Cargar dataset
        if use_streaming:
            dataset = load_dataset(dataset_name, dataset_config, split=split, streaming=True)
        else:
            if rank == 0:
                print("üì¶ Descargando dataset completo...")
            dataset = load_dataset(dataset_name, dataset_config, split=split)

        texts = []
        processed_count = 0
        current_sample = 0

        # Solo rank 0 muestra progreso
        if TQDM_AVAILABLE and rank == 0:
            progress = tqdm(enumerate(dataset), desc=f"Rank {rank}", total=end_sample)
        else:
            progress = enumerate(dataset)

        for i, item in progress:
            # ‚úÖ CORRECCI√ìN CLAVE: Saltar hasta llegar a nuestro rango
            if current_sample < start_sample:
                current_sample += 1
                continue
                
            # ‚úÖ CORRECCI√ìN CLAVE: Parar al final de nuestro rango
            if current_sample >= end_sample:
                break

            text = item.get(text_column, '')
            if isinstance(text, list):
                text = ' '.join(text)

            if text and len(text.strip()) >= min_text_length:
                text = text.strip()[:max_text_length]
                texts.append(text)
                processed_count += 1

                if TQDM_AVAILABLE and isinstance(progress, tqdm) and rank == 0:
                    progress.set_postfix({
                        'v√°lidos': processed_count,
                        'sample': current_sample,
                        'rango': f'{start_sample}-{end_sample}'
                    })
            
            current_sample += 1

        if TQDM_AVAILABLE and isinstance(progress, tqdm) and rank == 0:
            progress.close()

        print(f"‚úÖ Rank {rank}: {processed_count} textos √öNICOS cargados (samples {start_sample}-{end_sample})")

        if not texts:
            print(f"‚ùå Rank {rank}: Sin textos v√°lidos en rango {start_sample}-{end_sample}")
            raise ValueError(f"Sin datos v√°lidos para rank {rank}")

        return texts

    except Exception as e:
        print(f"‚ùå Error en rank {rank}: {e}")
        raise RuntimeError(f"Fall√≥ carga en rank {rank}: {e}") from e

# ======= DATASET CLASS CORREGIDA =======
class DistributedTextDatasetFixed(IterableDataset):
    """üîß DATASET CORREGIDO: No divide datos - cada rank ya tiene su porci√≥n √∫nica"""

    def __init__(self, tokenizer, texts: List[str], block_size: int = 256, split_type: str = "train",
                 device=None, max_length: int = 1024, min_text_length: int = 10, 
                 rank: int = 0, world_size: int = 1):
        self.tokenizer = tokenizer
        # ‚úÖ CORRECCI√ìN: NO dividir texts - ya son √∫nicos por rank
        self.local_texts = texts  # Usar directamente los texts √∫nicos
        self.block_size = block_size
        self.split_type = split_type
        self.device = device if device is not None else (torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu"))
        self.max_length = max_length
        self.min_text_length = min_text_length
        self.rank = rank
        self.world_size = world_size

        print(f"üìö üîß Dataset {split_type} Rank {rank}: {len(self.local_texts)} textos √öNICOS (NO divididos)")

    def __iter__(self):
        for text in self.local_texts:
            if not text or len(text.strip()) < self.min_text_length:
                continue

            try:
                tokens = self.tokenizer.encode(text, add_special_tokens=True,
                                             max_length=self.max_length, truncation=True)

                for i in range(0, len(tokens) - self.block_size + 1, self.block_size):
                    chunk = tokens[i:i + self.block_size]
                    if len(chunk) == self.block_size:
                        input_ids = torch.tensor(chunk[:-1], dtype=torch.long)
                        labels = torch.tensor(chunk[1:], dtype=torch.long)
                        yield {
                            'input_ids': input_ids,
                            'labels': labels
                        }
            except Exception as e:
                if self.rank == 0:
                    print(f"‚ö†Ô∏è Error tokenizando en rank {self.rank}: {e}")
                continue

# ======= FUNCI√ìN DE ENTRENAMIENTO CORREGIDA =======
def train_hrm_distributed_fixed():
    """üîß Funci√≥n de entrenamiento con distribuci√≥n de datos CORREGIDA"""
    
    # Setup distributed
    rank, world_size, local_rank, device = setup_distributed()
    is_main_process = rank == 0
    is_distributed = world_size > 1

    if is_main_process:
        print("üîß ENTRENAMIENTO DISTRIBUIDO CORREGIDO - HRM 50M")
        print(f"üåê World size: {world_size}, Rank: {rank}")

    # Crear tokenizador (solo ejemplo)
    tokenizer = create_tokenizer("openai-community/gpt2")

    # üîß CARGA CORREGIDA: cada rank carga solo SU porci√≥n
    train_texts = load_dataset_hf_fixed(
        tokenizer, "train", 
        num_samples=100000,  # Total samples
        dataset_name="allenai/c4",
        rank=rank, 
        world_size=world_size  # ‚úÖ Pasar world_size!
    )
    
    val_texts = load_dataset_hf_fixed(
        tokenizer, "validation", 
        num_samples=10000,
        dataset_name="allenai/c4", 
        rank=rank, 
        world_size=world_size  # ‚úÖ Pasar world_size!
    )

    # üîß DATASET CORREGIDO: no divisi√≥n adicional
    train_dataset = DistributedTextDatasetFixed(
        tokenizer, train_texts, 512, "train",
        rank=rank, world_size=world_size
    )
    
    val_dataset = DistributedTextDatasetFixed(
        tokenizer, val_texts, 512, "validation", 
        rank=rank, world_size=world_size
    )

    # Dataloaders (sin DistributedSampler - no necesario ahora)
    train_loader = DataLoader(
        train_dataset,
        batch_size=8,  # Por GPU
        shuffle=False,
        num_workers=0,
        pin_memory=device.type == 'cuda',
        drop_last=True,
    )

    print(f"‚úÖ Rank {rank}: Training setup completo con datos √öNICOS")
    print(f"   üìä Train texts: {len(train_texts)}")
    print(f"   üìä Val texts: {len(val_texts)}")

    # Resto del entrenamiento sigue igual...
    cleanup_distributed()

# Funci√≥n de validaci√≥n
def validate_data_distribution():
    """Funci√≥n para validar que la distribuci√≥n est√° funcionando"""
    print("üß™ Validando distribuci√≥n de datos...")
    
    # Simular 4 ranks
    total_samples = 1000
    world_size = 4
    
    for rank in range(world_size):
        samples_per_rank = total_samples // world_size
        start_sample = rank * samples_per_rank
        end_sample = start_sample + samples_per_rank
        
        if rank == world_size - 1:
            end_sample = total_samples
        
        print(f"Rank {rank}: samples {start_sample}-{end_sample} ({end_sample-start_sample} samples)")
    
    print("‚úÖ Validaci√≥n exitosa - cada rank procesa datos √∫nicos")

if __name__ == "__main__":
    print("üîß HRM 50M Distributed Training - VERSI√ìN CORREGIDA")
    print("\nüß™ Ejecutando validaci√≥n de distribuci√≥n:")
    validate_data_distribution()
    
    print("\nüí° Para usar la versi√≥n corregida:")
    print("1. Reemplazar las funciones en el script original")
    print("2. O usar este script como base")
    print("3. Ejecutar con: torchrun --nproc_per_node=N script.py")